{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "## Principal Component Analysis\n",
    "\n",
    "Here we will use SVD to compute PCA (the bedrock dimensionality reduction technique in Machine Learning). This is a statistical perspective on PCA. PCA gives us a hierarchical coordinate system, based on data, to represent the statistcal variation in your datast (the directions that capture the maximu amount of variability.)\n",
    "\n",
    "In this case we represent the data matrix $\\mathbf{X}$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\left[\n",
    "    \\begin{matrix}\n",
    "- & X_1 & -\\\\\n",
    "- & X_2 & -\\\\\n",
    "- & \\vdots & -\\\\\n",
    "    \\end{matrix}\n",
    "    \\right]\n",
    "$$\n",
    "\n",
    "where $X_i$ are _row_ vectors representing single experiments.\n",
    "\n",
    "## Steps for SVD\n",
    "1. Compute the \"average row\" and create a matrix $\\bar{X}$ by \"repeating\" this row\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n}\\sum_{j=1}^{n} x_j\\\\\n",
    "\\bar{X} = \\left[ \n",
    "    \\begin{matrix}\n",
    "    1\\\\\n",
    "    1\\\\\n",
    "    \\vdots\\\\\n",
    "    1\n",
    "\\end{matrix}\n",
    "\\right] \\left[\n",
    "    \\begin{matrix} \n",
    "    -&-&-&-&\\bar{x}&-&-&-&-\\\\\n",
    "    \\end{matrix}\n",
    "    \\right]\n",
    "$$\n",
    "2. Subtract the mean matrix from the original matrix and call it $B$ (mean centred matrix, this is zero mean gaussian)\n",
    "$$\n",
    "B = X - \\bar{X}\n",
    "$$\n",
    "3. Compute the _feature-wise_ covariance matrix of the rows of $B$:\n",
    "\n",
    "$$\n",
    "C = B^T B\n",
    "$$\n",
    "\n",
    "4. Compute the eigenvectors of $C$ which are the singular vectors of $X$\n",
    "$$\n",
    "V^T B^T B V = V^T C V\n",
    "$$\n",
    "Then we get:\n",
    "$$\n",
    "C V = V D\n",
    "$$\n",
    "where D is the eigenvalues matrix\n",
    "\n",
    "5. Nnow, the principal components are:\n",
    "\n",
    "$$\n",
    "T = BV\n",
    "$$\n",
    "where $V$ are the loadings of each of the principal components each of the experiments has.\n",
    "\n",
    "Now, remember that:\n",
    "\n",
    "$$\n",
    "B = U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "Then $T=U\\Sigma V^T\\;V = U \\Sigma$. Then $B = TV^T$. This means that you can get the principal component $T$ and the loadings $V$ from the SVD of a matrix. Obviously the eigenvalues in $D$ give us an idea about the amount of variance the corresponding eigenvetors capture. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "name": "julia",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7a70db9d531d6dd80c25b9f1d2725db6a2644ebf002a6e912882f79f6b1a032"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
